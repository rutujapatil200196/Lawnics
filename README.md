# Lawnics
CountVectorizer gives you a vector with the number of times each word appears in the document. This leads to a few problems mainly that common words like “a”,”the”,”and”,”an” etc will appear most of the time and other words that carry the topic information of your document will be less frequent.
 if you’re training a classifier to identify documents related to AI you don’t want it to learn words like “a” and “the” because they’ll be in every single document (both related to AI and not related documents). Furthermore the number of occurrences for these non topic bearing terms will be significantly higher than any other term. This will force them to have the highest weight in the model simply due to their high occurrence and will skew your model.

The way to combat this problem is to use TF-IDF. What TF-IDF does is it balances out the term frequency (how often the word appears in the document) with its inverse document frequency (how often the term appears across all documents in the data set). This means that words like “a” and “the” will have very low scores as they’ll appear in all documents in your set. Rarer words like for instance “machine learning” will be very common in just a handful of documents which talk about computer science or AI. TF-IDF will give higher scores to these words and thus they’ll be the ones that the model identifies as important and tries to learn.
